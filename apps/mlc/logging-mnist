[10:49:51] /home/zeonfaiho/tvm/src/runtime/logging.cc:390: TVM_LOG_DEBUG enables VLOG statements in 'ir/transform.cc' up to level 1
[10:49:51] /home/zeonfaiho/tvm/src/runtime/logging.cc:390: TVM_LOG_DEBUG enables VLOG statements in 'relay/ir/transform.cc' up to level 1
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass RemoveUnusedFunctions
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'RemoveUnusedFunctions'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass ToBasicBlockNormalForm
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'ToBasicBlockNormalForm'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass qnn.Legalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass QnnLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'QnnLegalize'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass QnnCanonicalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'QnnCanonicalize'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass SimplifyInference
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: SimplifyInference: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: SimplifyInference: Input module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: SimplifyInference: Output module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyInference: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass EliminateCommonSubexpr
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'EliminateCommonSubexpr'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass CombineParallelConv2d
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'CombineParallelConv2d'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass CombineParallelDense
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'CombineParallelDense'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass CombineParallelBatchMatmul
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'CombineParallelBatchMatmul'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FoldConstant
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'FoldConstant'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FoldScaleAxis
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass BackwardFoldScaleAxis
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'BackwardFoldScaleAxis'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass ForwardFoldScaleAxis
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'ForwardFoldScaleAxis'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FoldConstant
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'FoldConstant'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass SimplifyExpr
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: SimplifyExpr: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: SimplifyExpr: Input module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: SimplifyExpr: Output module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: SimplifyExpr: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass CanonicalizeCast
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'CanonicalizeCast'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass CanonicalizeOps
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'CanonicalizeOps'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FlattenAtrousConv
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: FlattenAtrousConv: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: FlattenAtrousConv: Input module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: FlattenAtrousConv: Output module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: FlattenAtrousConv: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FastMath
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'FastMath'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FoldConstant
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:481: Build: skipping disabled pass 'FoldConstant'
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass PlanDevices
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass PlanDevicesRewrite
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: PlanDevicesRewrite: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: PlanDevicesRewrite: Input module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: PlanDevicesRewrite: Output module:
def @main(%data: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.bias_add(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %4 = nn.relu(%3) /* ty=Tensor[(1, 128), float32] */;
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 128), float32] */;
  %6 = nn.dense(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0)) /* ty=Tensor[(1, 10), float32] */;
  %8 = nn.bias_add(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0), constrain_result=True)
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: PlanDevicesRewrite: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass PlanDevicesCheckConflicts
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: PlanDevicesCheckConflicts: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass InferType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass PlanDevicesCore
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: PlanDevicesCore: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass FuseOps
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: FuseOps: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: FuseOps: Input module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = nn.dense(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */, units=None) /* ty=Tensor[(1, 128), float32] */;
  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(1, 128), float32] */;
  %3 = nn.dense(%2, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */, units=None) /* ty=Tensor[(1, 10), float32] */;
  nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: FuseOps: Output module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32], %p13: Tensor[(128, 784), float32], Primitive=1) -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None)
  };
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */);
  %2 = fn (%p03: Tensor[(1, 128), float32], %p12: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12)
  };
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */);
  %4 = fn (%p02: Tensor[(1, 128), float32], Primitive=1) -> Tensor[(1, 128), float32] {
    nn.relu(%p02)
  };
  %5 = %4(%3);
  %6 = fn (%p01: Tensor[(1, 128), float32], %p11: Tensor[(10, 128), float32], Primitive=1) -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None)
  };
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */);
  %8 = fn (%p0: Tensor[(1, 10), float32], %p1: Tensor[(10), float32], Primitive=1) -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1)
  };
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */)
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: FuseOps: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InlineGlobals: Executing module pass with opt level: 1
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: LabelOps: Executing function pass with opt level: 1
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: LabelOps: Input module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1) -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1) -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1) -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1) -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1) -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: LabelOps: Output module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: LabelOps: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: AnnotateMemoryScope: Executing function pass with opt level: 2
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: AnnotateMemoryScope: Input module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: AnnotateMemoryScope: Output module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'executor' = graph{"link-params": T.bool(False)}
  'runtime' = cpp
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: AnnotateMemoryScope: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: Running pass RelayToTIRTargetHook
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: RelayToTIRTargetHook: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: Running pass LowerTE
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: LowerTE: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:124: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Executing function pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:125: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Input module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
}
attributes {
  'constant_memory_pools' = (nullptr)
  'executor' = graph{"link-params": T.bool(False)}
  'main_func_info' = FunctionInfoNode(
workspace_sizes={llvm -keys=cpu : 0, cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 1024},
  io_sizes={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 3176},
  constant_sizes={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 407080},
  tir_primfuncs={},
  relay_primfuncs={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: fn (%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
} /* ty=fn (Tensor[(1, 784), float32]) -> Tensor[(1, 10), float32] */
})
  'runtime' = cpp
  'workspace_memory_pools' = (nullptr)
}


One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LiftThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPermutedLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TransformMmaBufferLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FP8ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LiftThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPermutedLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TransformMmaBufferLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FP8ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LiftThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPermutedLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TransformMmaBufferLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FP8ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LiftThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPermutedLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TransformMmaBufferLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FP8ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LiftThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPermutedLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TransformMmaBufferLayout
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FP8ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR
[10:49:52] /home/zeonfaiho/tvm/src/relay/ir/transform.cc:148: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Output module:
def @main(%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = (%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */);
  %1 = call_lowered(@tvmgen_default_fused_nn_dense, %0, metadata={"relay_attrs"={__dict__={"Primitive"=1, "hash"="21395bd3c219bfce"}}, "all_prim_fn_vars"=['tvmgen_default_fused_nn_dense']});
  %2 = (%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */);
  %3 = call_lowered(@tvmgen_default_fused_nn_bias_add, %2, metadata={"relay_attrs"={__dict__={"Primitive"=1, "hash"="1e855c63ad1bcbb4"}}, "all_prim_fn_vars"=['tvmgen_default_fused_nn_bias_add']});
  %4 = (%3,);
  %5 = call_lowered(@tvmgen_default_fused_nn_relu, %4, metadata={"relay_attrs"={__dict__={"Primitive"=1, "hash"="bd99c8308cdaf0d5"}}, "all_prim_fn_vars"=['tvmgen_default_fused_nn_relu']});
  %6 = (%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */);
  %7 = call_lowered(@tvmgen_default_fused_nn_dense_1, %6, metadata={"relay_attrs"={__dict__={"Primitive"=1, "hash"="164daeaa32ba284b"}}, "all_prim_fn_vars"=['tvmgen_default_fused_nn_dense_1']});
  %8 = (%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */);
  call_lowered(@tvmgen_default_fused_nn_bias_add_1, %8, metadata={"relay_attrs"={__dict__={"Primitive"=1, "hash"="c7ad5bd15b2da050"}}, "all_prim_fn_vars"=['tvmgen_default_fused_nn_bias_add_1']})
}
attributes {
  'constant_memory_pools' = (nullptr)
  'executor' = graph{"link-params": T.bool(False)}
  'main_func_info' = FunctionInfoNode(
workspace_sizes={llvm -keys=cpu : 0, cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 1024},
  io_sizes={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 3176},
  constant_sizes={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: 407080},
  tir_primfuncs={},
  relay_primfuncs={cuda -keys=cuda,gpu -arch=sm_89 -max_num_threads=1024 -thread_warp_size=32: fn (%data {virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))}: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash="4a41e62083296ca5", virtual_device=VirtualDevice(device_type=2, virtual_device_id=0, target=Target(id=5634df4230e0, kind='cuda', keys={'cuda', 'gpu'}, attrs={'thread_warp_size': 32, 'max_num_threads': 1024, 'arch': "sm_89"}, host=Target(id=5634df7c72b0, kind='llvm', keys={'cpu'})))) -> Tensor[(1, 10), float32] {
  %0 = fn (%p04: Tensor[(1, 784), float32] /* ty=Tensor[(1, 784), float32] */, %p13: Tensor[(128, 784), float32] /* ty=Tensor[(128, 784), float32] */, Primitive=1, hash="21395bd3c219bfce") -> Tensor[(1, 128), float32] {
    nn.dense(%p04, %p13, units=None) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 784), float32], Tensor[(128, 784), float32]) -> Tensor[(1, 128), float32] */;
  %1 = %0(%data, meta[relay.Constant][0] /* ty=Tensor[(128, 784), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %2 = fn (%p03: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p12: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1, hash="1e855c63ad1bcbb4") -> Tensor[(1, 128), float32] {
    nn.bias_add(%p03, %p12) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(128), float32]) -> Tensor[(1, 128), float32] */;
  %3 = %2(%1, meta[relay.Constant][1] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;
  %4 = fn (%p02: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, Primitive=1, hash="bd99c8308cdaf0d5") -> Tensor[(1, 128), float32] {
    nn.relu(%p02) /* ty=Tensor[(1, 128), float32] */
  } /* ty=fn (Tensor[(1, 128), float32]) -> Tensor[(1, 128), float32] */;
  %5 = %4(%3) /* ty=Tensor[(1, 128), float32] */;
  %6 = fn (%p01: Tensor[(1, 128), float32] /* ty=Tensor[(1, 128), float32] */, %p11: Tensor[(10, 128), float32] /* ty=Tensor[(10, 128), float32] */, Primitive=1, hash="164daeaa32ba284b") -> Tensor[(1, 10), float32] {
    nn.dense(%p01, %p11, units=None) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 128), float32], Tensor[(10, 128), float32]) -> Tensor[(1, 10), float32] */;
  %7 = %6(%5, meta[relay.Constant][2] /* ty=Tensor[(10, 128), float32] */) /* ty=Tensor[(1, 10), float32] */;
  %8 = fn (%p0: Tensor[(1, 10), float32] /* ty=Tensor[(1, 10), float32] */, %p1: Tensor[(10), float32] /* ty=Tensor[(10), float32] */, Primitive=1, hash="c7ad5bd15b2da050") -> Tensor[(1, 10), float32] {
    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 10), float32] */
  } /* ty=fn (Tensor[(1, 10), float32], Tensor[(10), float32]) -> Tensor[(1, 10), float32] */;
  %8(%7, meta[relay.Constant][3] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */
} /* ty=fn (Tensor[(1, 784), float32]) -> Tensor[(1, 10), float32] */
})
  'runtime' = cpp
  'workspace_memory_pools' = (nullptr)
}


[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: Running pass InferType
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: GraphExecutorCodegen: Running pass tir.ExtractPrimFuncConstants
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: GraphExecutorCodegen: tir.ExtractPrimFuncConstants: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.calculate_allocated_bytes
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.calculate_allocated_bytes: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerVtcmAlloc
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.BindTarget
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.VerifyMemory
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.VerifyMemory: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.AnnotateEntryFunc
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.AnnotateEntryFunc: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.ThreadSync
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.ThreadSync
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.MergeDynamicSharedMemoryAllocations
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.ThreadSync
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.InferFragment
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerThreadAllreduce
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.AnnotateDeviceRegions
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.SplitHostDevice
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.SplitHostDevice: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.SplitHostDevice: tir.ConvertSSA: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.MakePackedAPI
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.MakePackedAPI: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.FP8StorageLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.BF16StorageLegalize
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerDeviceKernelLaunch
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:416: Build: tir.LowerDeviceKernelLaunch: Executing module pass with opt level: 0
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.Filter
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.BindTarget
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerTVMBuiltin
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerCustomDatatypes
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerIntrin
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerDeviceStorageAccessInfo
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.CombineContextCall
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.Filter
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.BindTarget
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerWarpMemory
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.Simplify
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerCustomDatatypes
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerDeviceStorageAccessInfo
[10:49:52] /home/zeonfaiho/tvm/src/ir/transform.cc:477: Build: Running pass tir.LowerIntrin
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/arith/int_set.cc:527: Warning: cannot evaluate set type tir.Call
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 3136 B, used memory 3136 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 401408 B, used memory 404544 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 512 B, used memory 405056 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 512 B, used memory 405568 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 512 B, used memory 406080 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 5120 B, used memory 411200 B
[10:49:52] /home/zeonfaiho/tvm/src/runtime/memory/naive_allocator.h:47: allocate 40 B, used memory 411240 B
Original main function:
free_var %data: Tensor[(1, 784), float32];
%0 = nn.dense(%data, meta[relay.Constant][0], units=None);
%1 = nn.bias_add(%0, meta[relay.Constant][1]);
%2 = nn.relu(%1);
%3 = nn.dense(%2, meta[relay.Constant][2], units=None);
nn.bias_add(%3, meta[relay.Constant][3])

<class 'tvm.relay.expr.Call'>
<class 'tvm.relay.expr.Call'>
<class 'tvm.relay.expr.Call'>
<class 'tvm.relay.expr.Call'>
<class 'tvm.relay.expr.Call'>
<class 'tvm.relay.expr.Var'>
<class 'tvm.relay.expr.Constant'>
<class 'tvm.relay.expr.Constant'>
<class 'tvm.relay.expr.Constant'>
<class 'tvm.relay.expr.Constant'>
New main function:
free_var %data: Tensor[(1, 784), float32];
%0 = nn.dense(%data, meta[relay.Constant][0], units=None);
%1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
%2 = nn.bias_add(%1, meta[relay.Constant][1]);
%3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
%4 = nn.relu(%3);
%5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
%6 = nn.dense(%5, meta[relay.Constant][2], units=None);
%7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
%8 = nn.bias_add(%7, meta[relay.Constant][3]);
on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0))

Transformed IRModule: def @main(%data: Tensor[(1, 784), float32]) {
  %0 = nn.dense(%data, meta[relay.Constant][0], units=None);
  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
  %2 = nn.bias_add(%1, meta[relay.Constant][1]);
  %3 = on_device(%2, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
  %4 = nn.relu(%3);
  %5 = on_device(%4, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
  %6 = nn.dense(%5, meta[relay.Constant][2], units=None);
  %7 = on_device(%6, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));
  %8 = nn.bias_add(%7, meta[relay.Constant][3]);
  on_device(%8, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0))
}


Output: [[-28.304365  -35.1353    -20.574156  -20.60154   -17.119598    2.7829068
  -15.253116    0.2125396  -4.941823    8.81127  ]]
Prediction: Ankle boot
